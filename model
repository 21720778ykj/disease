import torch
import torch.nn as nn
# from models.resnet_model import resnet34
import math
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from pytorch_pretrained_bert import BertTokenizer
from models.resnet_model import resnet152
from models.yolov4 import Yolov4, Conv_Bn_Activation, Upsample, ConvTranspose_Bn_Activation
from models.transformerEncoder import BertAttention
from config import Config
import torch.nn.functional as F
class ITFM(torch.nn.Module):
    def __init__(self, params,  embeddings, pretrain_model, pretrained_weight=None, num_of_tags=4):
        super(MSTD, self).__init__()
        self.params = params
        self.embeddings = embeddings
        self.input_embeddding_size = embeddings.embedding_length
        self.yolov4 = Yolov4(yolov4conv137weight=self.params.yolov4conv137weight, n_classes=103)
        self.conv_transformer1 = Conv_Transformer1(self.input_embeddding_size)
        self.conv_transformer2 = Conv_Transformer2(self.input_embeddding_size)
        self.conv_transformer3 = Conv_Transformer3(self.input_embeddding_size)
        self.dropout = nn.Dropout(params.dropout)
        self.self_att = BertAttention(config=Config)
    def forward(self, sentences, x_flair, sentence_lens, mask, chars, img,
                mode="train.txt"):  # !!! word_seq  char_seq
        # print(mask.shape)
        # print(images.shape)
        batch_size = img.shape[0]
        if mode == 'test':
            batch_size = 1
        attention_mask = torch.zeros((batch_size, 75+mask.shape[1]), dtype=torch.float)
        # attention_mask = torch.zeros((batch_size, 75), dtype=torch.float)
        img_mask = torch.ones((batch_size, 75), dtype=torch.float)
        attention_mask[:, :75] = img_mask
        attention_mask[:, 75:] = mask
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype, device=next(self.parameters()).device)  # fp16 compatibility
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        d5, d4, d3 = self.yolov4.encoder(img)  # 1024*19*19 512*38*38  256*76*76
        t5 = self.conv_transformer1.c2t(d5)
        t4 = self.conv_transformer2.c2t(d4)
        t3 = self.conv_transformer3.c2t(d3)
        t5 = t5.view(batch_size, self.input_embeddding_size, 5 * 5).transpose(2, 1)
        t4 = t4.view(batch_size, self.input_embeddding_size, 5 * 5).transpose(2, 1)
        t3 = t3.view(batch_size, self.input_embeddding_size, 5 * 5).transpose(2, 1)
        t0 = torch.cat((t5, t4, t3), dim=1)
        self.embeddings.embed(x_flair)
        lengths = [len(sentence.tokens) for sentence in x_flair]
        longest_token_sequence_in_batch: int = max(lengths)
        pre_allocated_zero_tensor = torch.zeros(
            self.input_embeddding_size * longest_token_sequence_in_batch,
            dtype=torch.float,
        )
        all_embs = list()
        for sentence in x_flair:
            all_embs += [
                emb for token in sentence for emb in token.get_each_embedding()
            ]
            nb_padding_tokens = longest_token_sequence_in_batch - len(sentence)
            if nb_padding_tokens > 0:
                t = pre_allocated_zero_tensor[
                    : self.input_embeddding_size * nb_padding_tokens
                    ].cuda()
                all_embs.append(t)

        embed_flair = torch.cat(all_embs).view(
            [
                len(x_flair),
                longest_token_sequence_in_batch,
                self.input_embeddding_size,
            ]
        )

        embeds = self.dropout(embed_flair)

        embeds = torch.cat((t0, embeds), dim=1)

        embeds = self.self_att(embeds, extended_attention_mask)
        c = embeds[:, :75, :]
        txt_feature = embeds[:, 75:, :]
        c5 = c[:, :25, :]
        c4 = c[:, 25:50, :]
        c3 = c[:, 50:, :]
        c5 = c5.transpose(2, 1).view(batch_size, self.input_embeddding_size, 5, 5)
        c4 = c4.transpose(2, 1).view(batch_size, self.input_embeddding_size, 5, 5)
        c3 = c3.transpose(2, 1).view(batch_size, self.input_embeddding_size, 5, 5)
        t5 = self.conv_transformer1.t2c(c5)
        t4 = self.conv_transformer2.t2c(c4)
        t3 = self.conv_transformer3.t2c(c3)
        img_output = self.yolov4.decoder(t5, t4, t3)

        return  img_output  

